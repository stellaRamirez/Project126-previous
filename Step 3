---
title: "Step-3"
author: "Liuqian Bao"
date: "2023-11-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("skimr")
library("dplyr")
options(max.print=1000000)
library("readxl")
library("ggplot2")
# load packages
library("tidyverse")
library("tidymodels")
library("modelr")
library("ggplot2")
library("GGally")
train <- read_excel("C:/Users/baoli/Desktop/PSTAT126/Project/house-prices-advanced-regression-techniques/train.xlsx") 
```

```{r include=FALSE}
# Variable Selection
h1 <- subset(train, select = -c(MSSubClass, OverallCond, BsmtFinSF2, LowQualFinSF, BsmtHalfBath, YrSold, MoSold, MiscVal, PoolArea, ScreenPorch, EnclosedPorch, KitchenAbvGr, BedroomAbvGr, Street, Alley, Utilities, LandSlope, RoofMatl, BsmtExposure, Heating, CentralAir, MiscFeature, Fence, PoolQC, LandContour,BldgType, Exterior2nd, GarageQual, GarageCond, PavedDrive, SaleType, SaleCondition, GrLivArea, OverallQual, TotalBsmtSF, WoodDeckSF, GarageArea, Fireplaces, TotRmsAbvGrd, MSZoning, Functional, MasVnrArea, GarageYrBlt, YearBuilt, YearRemodAdd))

h2 <- subset(train, select = c(SalePrice, LotArea, GarageCars, BsmtFinSF1, FullBath, ExterQual, HouseStyle, OpenPorchSF, HalfBath, KitchenQual, GarageFinish))
```

```{r echo=F}
# randomly choose 500 observations
set.seed(12345)

h2_500 <- h2[sample(1:1460, 500, replace = FALSE),]
h2_partition <- h2_500 %>% resample_partition(p = c(train = 0.7, test = 0.3)) ##fit model
h3_500 <- subset(h2_500, select = c(SalePrice, LotArea, GarageCars, BsmtFinSF1, FullBath, ExterQual)) ##pairplot
h4_500 <- subset(h2_500, select = c(HouseStyle, OpenPorchSF, HalfBath, KitchenQual, GarageFinish)) ##pairplot
```

```{r}
ggpairs(train3_500)
ggpairs(train4_500)

# plot(pairplot[2,1])
# plot(pairplot[2,2])
# plot(pairplot[2,3])
# plot(pairplot[2,4])
# plot(pairplot[2,5])
# plot(pairplot[2,6])
# plot(pairplot[2,7])
# plot(pairplot[2,8])
# plot(pairplot[2,9])
# plot(pairplot[2,10])
```
## Computational Models
First computational model: all of our quantitative variables.
```{r}
fitComp1 <- lm(log(SalePrice) ~ LotArea + GarageCars + BsmtFinSF1 + FullBath + OpenPorchSF + HalfBath, data = h2_partition$train)
summary(fitComp1)
```

```{r, echo=F, fig.width=8, fig.height=8, fig.cap="Residual vs. fitted and BsmtFinSF1"}
# panel of residual plots
augment(fitComp1, h2_partition$train) %>%
  pivot_longer(cols = c(.fitted, LotArea, GarageCars, BsmtFinSF1, FullBath, OpenPorchSF, HalfBath)) %>%
  ggplot(aes(y = .resid, x = value)) +
  facet_wrap(~ name, scales = 'free_x') +
  geom_point() +
  geom_hline(aes(yintercept = 0)) +
  geom_smooth(method = 'loess', formula = 'y ~ x', se = F, span = 1)
```

We tranformed SalePrice with a log function to make the variance more constant. 

Second computational model: we used GarageCars because it looked like it had high correlation, and KitchenQual becuase we wanted to test a highly correlated categorical variable. 

```{r}
fitComp2 <- lm(log(SalePrice) ~ GarageCars + KitchenQual, data = h2_partition$train)
summary(fitComp2)
```

We tranformed SalePrice with a log function to make the variance more constant.

## Anova to Cross Validate

```{r}
anova(fitComp1, fitComp2)
```

(we can also compare r^2 values)


#### Statistical model

```{r}
model_null = lm(SalePrice ~ 1, data = h2_partition$train)
model_full = lm(SalePrice ~ ., data = h2_partition$train)
# need to specify the scope of models to be examined
stats::step(model_full, direction="backward", test="F")
```

```{r}
stats::step(model_null, direction="forward", scope=list(upper=model_full, lower=model_null))
```
We performed both forward and backward selection, keeping the response as the log of SalePrice. We used the AIC as the criteria. The model returned will hace the predictors with the lowest AIC's. Each of these methods of selection returned the same model with 9 predictors, so we will use that as our statistical model.

(Using his method from lecture)
```{r, include=FALSE}
library(leaps)
out <- regsubsets(log(SalePrice) ~ ., data = h2_partition$train, # just like lm()
method = 'seqrep', # search strategy
nbest = 1, # how many models of each size?
nvmax = 10) # maximum number of predictors

summary(out)
```

### After Chose Model, fit it
```{r}
fitStat <- lm(log(SalePrice) ~ ExterQual + LotArea + GarageCars + BsmtFinSF1 + HalfBath + FullBath + KitchenQual + OpenPorchSF + GarageFinish, data = h2_partition$train)
summary(fitStat)
```

Looking at our R^2 value for this model shows that 78% of the variation in the response is explained by this model.

The following shows the residual plots we investigated to ensure that taking the log of the response helps with the model assumptions. 

```{r, echo=F, fig.width=8, fig.height=8, fig.cap="Residual vs. fitted and BsmtFinSF1"}
# panel of residual plots
augment(fitStat, h2_partition$train) %>%
  pivot_longer(cols = c(.fitted, LotArea, GarageCars, BsmtFinSF1, HalfBath, FullBath, OpenPorchSF)) %>%
  ggplot(aes(y = .resid, x = value)) +
  facet_wrap(~ name, scales = 'free_x') +
  geom_point() +
  geom_hline(aes(yintercept = 0)) +
  geom_smooth(method = 'loess', formula = 'y ~ x', se = F, span = 1)
```

### Reporting on Test Data

```{r}
fitTest <- lm(log(SalePrice) ~ ExterQual + LotArea + GarageCars + BsmtFinSF1 + HalfBath + FullBath + KitchenQual + OpenPorchSF + GarageFinish, data = h2_partition$test)
summary(fitTest)
```
### Looking for Influence points

```{r}
augment(fitTest, h2_partition$test) %>%
  mutate(obs_index = row_number()) %>%
  ggplot(aes(x=obs_index, y = .resid)) +
  geom_point() +
  geom_hline(aes(yintercept = 0)) + # add line at zero
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) # rotates and aligns labels
```
```{r, echo = T, fig.width = 10, fig.height = 8}
p_caseinf <- augment(fitTest, h2_partition$test) %>%
  pivot_longer(cols = c(.resid, .hat, .cooksd)) %>%
  mutate(obs_index = row_number()) %>%
  ggplot(aes(x=obs_index, y = value)) +
  facet_wrap(~ name, scales = 'free_y', nrow = 3) + # looks better with vertical faceting
  geom_point() +
  geom_hline(aes(yintercept = 0)) + # add line at zero
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) + # rotates and aligns labels
  labs(x = '', y = '') 

p_caseinf
```

```{r}
unusual_obs <- augment(fitTest, h2_partition$test) %>% 
  pivot_longer(cols = c(.resid, .hat, .cooksd)) %>%
  group_by(name) %>%
  slice_max(order_by = abs(value), n = 1) %>%
  ungroup()

p_caseinf + geom_point(data = unusual_obs, color = 'red')
```
