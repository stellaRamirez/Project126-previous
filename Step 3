---
title: "Step-3"
author: "Liuqian Bao"
date: "2023-11-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("skimr")
library("dplyr")
options(max.print=1000000)
library("readxl")
library("ggplot2")
# load packages
library("tidyverse")
library("tidymodels")
library("modelr")
library("ggplot2")
library("GGally")
train <- read_excel("C:/Users/baoli/Desktop/PSTAT126/Project/house-prices-advanced-regression-techniques/train.xlsx") 
```

```{r include=FALSE}
# Variable Selection
h1 <- subset(train, select = -c(MSSubClass, OverallCond, BsmtFinSF2, LowQualFinSF, BsmtHalfBath, YrSold, MoSold, MiscVal, PoolArea, ScreenPorch, EnclosedPorch, KitchenAbvGr, BedroomAbvGr, Street, Alley, Utilities, LandSlope, RoofMatl, BsmtExposure, Heating, CentralAir, MiscFeature, Fence, PoolQC, LandContour,BldgType, Exterior2nd, GarageQual, GarageCond, PavedDrive, SaleType, SaleCondition, GrLivArea, OverallQual, TotalBsmtSF, WoodDeckSF, GarageArea, Fireplaces, TotRmsAbvGrd, MSZoning, Functional, MasVnrArea, GarageYrBlt, YearBuilt, YearRemodAdd))

h2 <- subset(train, select = c(SalePrice, LotArea, GarageCars, BsmtFinSF1, FullBath, ExterQual, HouseStyle, OpenPorchSF, HalfBath, KitchenQual, GarageFinish))
```

```{r echo=F}
# randomly choose 500 observations
set.seed(12345)

h2_500 <- h2[sample(1:1460, 500, replace = FALSE),]
h2_partition <- h2_500 %>% resample_partition(p = c(train = 0.7, test = 0.3)) ##fit model
h3_500 <- subset(h2_500, select = c(SalePrice, LotArea, GarageCars, BsmtFinSF1, FullBath, ExterQual)) ##pairplot
h4_500 <- subset(h2_500, select = c(HouseStyle, OpenPorchSF, HalfBath, KitchenQual, GarageFinish)) ##pairplot
```

#### Introduction

```{r}
#ggpairs(train3_500)
#ggpairs(train4_500)

# plot(pairplot[2,1])
# plot(pairplot[2,2])
# plot(pairplot[2,3])
# plot(pairplot[2,4])
# plot(pairplot[2,5])
# plot(pairplot[2,6])
# plot(pairplot[2,7])
# plot(pairplot[2,8])
# plot(pairplot[2,9])
# plot(pairplot[2,10])
```
#### Computational Models

##### 1: 

For our first computational model, we chose to fit a model with all of our quantitative variables. 

```{r echo=FALSE}
fitComp1 <- lm(log(SalePrice) ~ LotArea + GarageCars + BsmtFinSF1 + FullBath + OpenPorchSF + HalfBath, data = h2_partition$train)
summary(fitComp1)
```

In the following Graphs, we plotted the residuals for each variable to assess the model fit. We performed this twice, concluding that it is necessary to take the log of the response in order to correct the constant variance assumption. We have omitted the graphs of the un-transformed model, opting to display those of the model more useful to us.

```{r, echo=F, fig.width=8, fig.height=8, fig.cap="Residual Graphs for Quantitative Variables"}
# panel of residual plots
augment(fitComp1, h2_partition$train) %>%
  pivot_longer(cols = c(.fitted, LotArea, GarageCars, BsmtFinSF1, FullBath, OpenPorchSF, HalfBath)) %>%
  ggplot(aes(y = .resid, x = value)) +
  facet_wrap(~ name, scales = 'free_x') +
  geom_point() +
  geom_hline(aes(yintercept = 0)) +
  geom_smooth(method = 'loess', formula = 'y ~ x', se = F, span = 1)
```
\newpage

##### Normality Check for Computational Model 1: \newline
Then, we performed a normality check, to make sure the model was adequate.

```{r, echo=FALSE, fig.width=4, fig.height=3, fig.cap="Normality Check for Computational Model 1"}
# normality check
augment(fitComp1, h2_partition$train) %>%
  ggplot(aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line()
```

##### 2:

For our second computational model, we used the variable GarageCars because it had high correlation, as seen in our beginning plots, and KitchenQual becuase we wanted to test a highly correlated categorical variable, in addition to quantitative.

```{r echo=FALSE}
fitComp2 <- lm(log(SalePrice) ~ GarageCars + KitchenQual, data = h2_partition$train)
summary(fitComp2)
```

Again, we transformed SalePrice with a log function to make the variance more constant. Then we performed the following normality check on this model as well.

##### Normality Check for Computational Model 2 \newline

```{r, echo=FALSE, fig.width=4, fig.height=3, fig.cap="Normality Check for Computational Model 2"}
# normality check
augment(fitComp2, h2_partition$train) %>%
  ggplot(aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line()
```

#### Cross Analyzing the Two Computational Models

##### Anova Function to Cross Validate

```{r echo=FALSE}
anova(fitComp1, fitComp2)
```

(we can also compare r^2 values)


#### Statistical Model \newline

```{r, include=FALSE}
library(leaps) # this is using his method from lexture, probably won't include in final report
out <- regsubsets(log(SalePrice) ~ ., data = h2_partition$train, # just like lm()
method = 'seqrep', # search strategy
nbest = 1, # how many models of each size?
nvmax = 10) # maximum number of predictors

summary(out)
```

Next, we used backward selection to assess which variables would be best to use in our model. To do so, we begin with a full model and subtract the predictors one at a time.

```{r echo=FALSE}
model_null = lm(SalePrice ~ 1, data = h2_partition$train)
model_full = lm(SalePrice ~ ., data = h2_partition$train)
# need to specify the scope of models to be examined
stats::step(model_full, direction="backward", test="F")
```
After doing so, we decided to also try forward selection to assess which variables would be best to use in our model. To do so, we begin with an empty model and add the predictors one at a time.

```{r echo=FALSE}
stats::step(model_null, direction="forward", scope=list(upper=model_full, lower=model_null))
```
In order to perform these two types of selection, keeping the response as the log of SalePrice, we used the AIC as the criteria. The model returned will have the predictors with the lowest AIC's. Both of these methods of selection returned the same model with 9 predictors, so we will use that as our statistical model.


The following displays the summary of our fitted statistical model. As you can see, the variables chosen were ExterQual, LotArea, GarageCars, BsmtFinSF1, HalfBath, FullBath, KitchenQual, OpenPorchSF, and GarageFinish. 
```{r echo=FALSE}
fitStat <- lm(log(SalePrice) ~ ExterQual + LotArea + GarageCars + BsmtFinSF1 + HalfBath + FullBath + KitchenQual + OpenPorchSF + GarageFinish, data = h2_partition$train)
summary(fitStat)
```


Looking at our R^2 value for this model shows that 78% of the variation in the response is explained by this model.

The following shows the residual plots we investigated to ensure that taking the log of the response helps with the model assumptions. 

\newpage
```{r, echo=F, fig.width=8, fig.height=8, fig.cap="Residual Graphs for the Statistical Model"}
# panel of residual plots
augment(fitStat, h2_partition$train) %>%
  pivot_longer(cols = c(.fitted, LotArea, GarageCars, BsmtFinSF1, HalfBath, FullBath, OpenPorchSF)) %>%
  ggplot(aes(y = .resid, x = value)) +
  facet_wrap(~ name, scales = 'free_x') +
  geom_point() +
  geom_hline(aes(yintercept = 0)) +
  geom_smooth(method = 'loess', formula = 'y ~ x', se = F, span = 1)
```


```{r, echo=FALSE, fig.width=4, fig.height=3, fig.cap="Normality Check for Statistical Model"}
# normality check
augment(fitStat, h2_partition$train) %>%
  ggplot(aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line()
```

#### Reporting on Test Data \newline

In the following section, we fit our test data with the linear model based on the predictors chosen from our selection process.

```{r}
fitTest <- lm(log(SalePrice) ~ ExterQual + LotArea + GarageCars + BsmtFinSF1 + HalfBath + FullBath + KitchenQual + OpenPorchSF + GarageFinish, data = h2_partition$test)
summary(fitTest)
```
Just in case, we performed a normality check to make sure the assumption holds within our test data. 

```{r, echo=FALSE, fig.width=4, fig.height=3, fig.cap="Normality Check for the Test Data Model"}
# normality check
augment(fitTest, h2_partition$test) %>%
  ggplot(aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line()
```

\newpage

#### Looking for Influence points \newline


##### Plot of the Rows and Their Residuals \newline
The following shows the row values plotted with their respective residuals. 

```{r echo=FALSE, fig.cap="Rows with Residuals"}
resid <- augment(fitTest, h2_partition$test) %>% 
  mutate(obs_index = row_number()) %>%
  ggplot(aes(x=obs_index, y = .resid)) +
  geom_point() +
  geom_hline(aes(yintercept = 0)) + # add line at zero
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) # rotates and aligns labels

resid
```

\newpage

##### Plot of the Rows and Their .hat Values \newline

This graph shows the row values with their .hat values. We use these graphs to investigate influence points.

```{r echo=FALSE, fig.cap="Rows with .hat"}
hat <- augment(fitTest, h2_partition$test) %>% 
  mutate(obs_index = row_number()) %>%
  ggplot(aes(x=obs_index, y = .hat)) +
  geom_point() +
  geom_hline(aes(yintercept = 0)) + # add line at zero
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) # rotates and aligns labels

hat
```
Next, we wanted to pull out the highest values for each .cooksd, .resid, and .hat, as we can potentially use these to indicate influential points.

```{r, echo = F}
augment(fitTest, h2_partition$test) %>% 
  pivot_longer(cols = c(.resid, .hat, .cooksd)) %>%
  group_by(name) %>%
  slice_max(order_by = abs(value), n = 1) %>% 
  select(name, value)
```

```{r, echo = F, fig.width = 8, include=FALSE}
augment(fitTest) #row 6 .hat, row 51 resid
```
After investigation, we found that the data with the highest .hat value is located in row 6 of our test data, and the data with the highest .resid is located in the 51st row of our test data.


#### Confidence Interval for Mean Response 

Next, we computed a confidence interval for the mean response. 

```{r}
h2_testDF <- as.data.frame(h2_partition$test)
x_bar <- h2_testDF %>% select(c(LotArea, GarageCars, BsmtFinSF1, HalfBath, FullBath, OpenPorchSF)) %>% summarise(across(everything(), mean))
# finding most frequent values for the categorical variables 
names(which.max(table(h2_testDF$ExterQual)))
names(which.max(table(h2_testDF$KitchenQual)))
names(which.max(table(h2_testDF$GarageFinish)))
# use the most frequent values in the mean prediction 
x_bar$ExterQual <- "TA"
x_bar$KitchenQual <- "TA"
x_bar$GarageFinish <- "Unf"
predict(fitTest, newdata = x_bar, interval = 'confidence', level = 0.95)
```

#### Prediction Interval At a Value 

Finally, we computed prediction intervals for a few points. The first is at a randomly chosen row, row 100. The next two are rows 6 and 51, the points of influence.

```{r}
x_point <- h2_testDF[100, ]
predict(fitTest, newdata = x_point , interval = 'prediction', level = 0.95)
x_InfluencePt1 <- h2_testDF[6, ]
predict(fitTest, newdata = x_InfluencePt1 , interval = 'prediction', level = 0.95)
x_InfluencePt2 <- h2_testDF[51, ]
predict(fitTest, newdata = x_InfluencePt2 , interval = 'prediction', level = 0.95)
```
As you can see, the points of influence affect the prediction intervals slightly. 

For step 3 of our project, we first put aside test data and introduced the reader to our data source. 
Before we started our analysis of the data, we created the pairs plot, and from the pairs plot, we concluded that the data have a moderate correlation. 
For our model, we have used the log transformation in order to obtain better correlation and stronger, more reliable relationships with our explanatory variables by through the presence of a more constant variance. 
We have then created an ANOVA table to cross-validate our data, and we are not using interaction variables. 
Then we would choose two models that are of interest to us, which would be model_null and model_full. 
Keeping our response the log of SalePrice, we have performed forward and backward selection using AIC as our criteria. 
Both forward and backward selection returned us the same model that has the same 9 predictors, so we decided to use that as our model. 
After choosing our model, we have fit the model and looked at the corresponding R^2 value, which showed that 78% variation in the response was explained by the model we chose. 
In order to get better interpretation of the model, we created residual plots for our predictors to make sure that taking the log of our response SalePrice helps with our model assumptions.   
We then fitted our model and then we looked for influence points. We used techniques including vertical faceting to make our models look better, and we concluded that from our plots, we can see an outlier point that is far apart from the rest of the data. 
Due to the potential negative influence of the outlier on our overall model, we have fitted a model by removing unwanted data points and concluded that the new fitted model worked much better for the training and the test datasets. 
Due to the diversity of our predictors, we do have some predictors that are categorial variables which contain various levels, and these categorial variables had distinctive influences on our model. 
They also had influence for the mean of the confidence intervals we tested. Finally, we gave confidence intervals for a mean predicted value and a prediction interval for a future predicted value. 
For both the confidence interval and the prediction interval, we used the predict function, and we have used the mean of our dataset through selecting some notable variables for our mean confidence interval. 
For our mean confidence interval, we have used the variable ExterQual as "TA", the variable KitchenQual as "TA", and the variable GarageFinish as "Unf". The confidence inveral data for "TA", "TA", and "Unf", we have the fit as 11.8618, the lower bound as 11.80702, and the upper bound as 11.91658.
The values we obtained from the confidence and prediction intervals were within our expectations in accordance with our model. 
Overall, we believed that the model we chose we a good fit for our data in step 3 of our project.
