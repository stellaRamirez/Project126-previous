---
title: "Step-3"
author: "Liuqian Bao"
date: "2023-11-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("skimr")
library("dplyr")
options(max.print=1000000)
library("readxl")
library("ggplot2")
# load packages
library("tidyverse")
library("tidymodels")
library("modelr")
library("ggplot2")
library("GGally")
train <- read_excel("C:/Users/baoli/Desktop/PSTAT126/Project/house-prices-advanced-regression-techniques/train.xlsx") 
```

```{r include=FALSE}
# Variable Selection
h1 <- subset(train, select = -c(MSSubClass, OverallCond, BsmtFinSF2, LowQualFinSF, BsmtHalfBath, YrSold, MoSold, MiscVal, PoolArea, ScreenPorch, EnclosedPorch, KitchenAbvGr, BedroomAbvGr, Street, Alley, Utilities, LandSlope, RoofMatl, BsmtExposure, Heating, CentralAir, MiscFeature, Fence, PoolQC, LandContour,BldgType, Exterior2nd, GarageQual, GarageCond, PavedDrive, SaleType, SaleCondition, GrLivArea, OverallQual, TotalBsmtSF, WoodDeckSF, GarageArea, Fireplaces, TotRmsAbvGrd, MSZoning, Functional, MasVnrArea, GarageYrBlt, YearBuilt, YearRemodAdd))

h2 <- subset(train, select = c(SalePrice, LotArea, GarageCars, BsmtFinSF1, FullBath, ExterQual, HouseStyle, OpenPorchSF, HalfBath, KitchenQual, GarageFinish))
```

```{r echo=F}
# randomly choose 500 observations
set.seed(12345)

h2_500 <- h2[sample(1:1460, 500, replace = FALSE),]
h2_partition <- h2_500 %>% resample_partition(p = c(train = 0.7, test = 0.3)) ##fit model
h3_500 <- subset(h2_500, select = c(SalePrice, LotArea, GarageCars, BsmtFinSF1, FullBath, ExterQual)) ##pairplot
h4_500 <- subset(h2_500, select = c(HouseStyle, OpenPorchSF, HalfBath, KitchenQual, GarageFinish)) ##pairplot
```

#### Introduction

```{r echo=FALSE, include=FALSE}
#ggpairs(train3_500)
#ggpairs(train4_500)

# plot(pairplot[2,1])
# plot(pairplot[2,2])
# plot(pairplot[2,3])
# plot(pairplot[2,4])
# plot(pairplot[2,5])
# plot(pairplot[2,6])
# plot(pairplot[2,7])
# plot(pairplot[2,8])
# plot(pairplot[2,9])
# plot(pairplot[2,10])
```
By looking at the pairs plots, we found that:
1. The explanatory variables FullBath and GarageCars are moderately correlated(Corr: 0.457***).
2. The explanatory variables ExterQual(exterior quality) and BsmtFinSF1(basement finished area) are somewhat correlated (judged by their box plots).

#### Computational Models

##### 1: 

For our first computational model, we chose to fit a model with all of our quantitative variables. 

```{r echo=FALSE}
fitComp1 <- lm(log(SalePrice) ~ LotArea + GarageCars + BsmtFinSF1 + FullBath + OpenPorchSF + HalfBath, data = h2_partition$train)
summary(fitComp1)
```

In the following Graphs, we plotted the residuals for each variable to assess the model fit. We performed this twice, concluding that it is necessary to take the log of the response in order to correct the constant variance assumption. We have omitted the graphs of the un-transformed model, opting to display those of the model more useful to us.

```{r, echo=F, fig.width=8, fig.height=8, fig.cap="Residual Graphs for Quantitative Variables"}
# panel of residual plots
augment(fitComp1, h2_partition$train) %>%
  pivot_longer(cols = c(.fitted, LotArea, GarageCars, BsmtFinSF1, FullBath, OpenPorchSF, HalfBath)) %>%
  ggplot(aes(y = .resid, x = value)) +
  facet_wrap(~ name, scales = 'free_x') +
  geom_point() +
  geom_hline(aes(yintercept = 0)) +
  geom_smooth(method = 'loess', formula = 'y ~ x', se = F, span = 1)
```
\newpage

##### Normality Check for Computational Model 1: \newline
Then, we performed a normality check, to make sure the model was adequate.

```{r, echo=FALSE, fig.width=4, fig.height=3, fig.cap="Normality Check for Computational Model 1"}
# normality check
augment(fitComp1, h2_partition$train) %>%
  ggplot(aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line()
```

##### 2:

For our second computational model, we used the variable GarageCars because it had high correlation, as seen in our beginning plots, and KitchenQual becuase we wanted to test a highly correlated categorical variable, in addition to quantitative.

```{r echo=FALSE}
fitComp2 <- lm(log(SalePrice) ~ GarageCars + KitchenQual, data = h2_partition$train)
summary(fitComp2)
```

Again, we transformed SalePrice with a log function to make the variance more constant. Then we performed the following normality check on this model as well.

##### Normality Check for Computational Model 2 \newline

```{r, echo=FALSE, fig.width=4, fig.height=3, fig.cap="Normality Check for Computational Model 2"}
# normality check
augment(fitComp2, h2_partition$train) %>%
  ggplot(aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line()
```

#### Cross Analyzing the Two Computational Models

##### Anova Function to Cross Validate\newline

```{r echo=FALSE}
anova(fitComp1, fitComp2)
```

The anova test results in very small p-value, which allows us to conclude that model 1, the model with all of our numerical variables, are significantly better than the other model.

##### Adjusted $R^2$ to Cross Validate \newline

The adjusted $R^2$ of model 1 is 0.6935, which means that the model explains 69.35% of the variation in the response is explained by model 1. On the other hand, the adjusted $R^2$ of model 2 is 0.5876, which tells us that model 1 explains more variation in the response. Judged by this, model 1 is the better model. 


#### Statistical Model \newline

```{r, include=FALSE}
library(leaps) # this is using his method from lexture, probably won't include in final report
out <- regsubsets(log(SalePrice) ~ ., data = h2_partition$train, # just like lm()
method = 'seqrep', # search strategy
nbest = 1, # how many models of each size?
nvmax = 10) # maximum number of predictors

summary(out)
```

Next, we used backward selection to assess which variables would be best to use in our model. To do so, we begin with a full model and subtract the predictors one at a time.

```{r echo=FALSE}
model_null = lm(SalePrice ~ 1, data = h2_partition$train)
model_full = lm(SalePrice ~ ., data = h2_partition$train)
# need to specify the scope of models to be examined
stats::step(model_full, direction="backward", test="F")
```
After doing so, we decided to also try forward selection to assess which variables would be best to use in our model. To do so, we begin with an empty model and add the predictors one at a time.

```{r echo=FALSE}
stats::step(model_null, direction="forward", scope=list(upper=model_full, lower=model_null))
```
In order to perform these two types of selection, keeping the response as the log of SalePrice, we used the AIC as the criteria. The model returned will have the predictors with the lowest AIC's. Both of these methods of selection returned the same model with 9 predictors, so we will use that as our statistical model.


The following displays the summary of our fitted statistical model. As you can see, the variables chosen were ExterQual, LotArea, GarageCars, BsmtFinSF1, HalfBath, FullBath, KitchenQual, OpenPorchSF, and GarageFinish. 
```{r echo=FALSE}
fitStat <- lm(log(SalePrice) ~ ExterQual + LotArea + GarageCars + BsmtFinSF1 + HalfBath + FullBath + KitchenQual + OpenPorchSF + GarageFinish, data = h2_partition$train)
summary(fitStat)
```


Looking at our R^2 value for this model shows that 78% of the variation in the response is explained by this model.

The following shows the residual plots we investigated to ensure that taking the log of the response helps with the model assumptions. 

\newpage
```{r, echo=F, fig.width=8, fig.height=8, fig.cap="Residual Graphs for the Statistical Model"}
# panel of residual plots
augment(fitStat, h2_partition$train) %>%
  pivot_longer(cols = c(.fitted, LotArea, GarageCars, BsmtFinSF1, HalfBath, FullBath, OpenPorchSF)) %>%
  ggplot(aes(y = .resid, x = value)) +
  facet_wrap(~ name, scales = 'free_x') +
  geom_point() +
  geom_hline(aes(yintercept = 0)) +
  geom_smooth(method = 'loess', formula = 'y ~ x', se = F, span = 1)
```


```{r, echo=FALSE, fig.width=4, fig.height=3, fig.cap="Normality Check for Statistical Model"}
# normality check
augment(fitStat, h2_partition$train) %>%
  ggplot(aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line()
```

#### Reporting on Test Data \newline

In the following section, we fit our test data with the linear model based on the predictors chosen from our selection process.

```{r echo=FALSE}
fitTest <- lm(log(SalePrice) ~ ExterQual + LotArea + GarageCars + BsmtFinSF1 + HalfBath + FullBath + KitchenQual + OpenPorchSF + GarageFinish, data = h2_partition$test)
summary(fitTest)
```
Just in case, we performed a normality check to make sure the assumption holds within our test data. 

```{r, echo=FALSE, fig.width=4, fig.height=3, fig.cap="Normality Check for the Test Data Model"}
# normality check
augment(fitTest, h2_partition$test) %>%
  ggplot(aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line()
```

\newpage

#### Looking for Influence points \newline


##### Plot of the Rows and Their Residuals \newline
The following shows the row values plotted with their respective residuals. 

```{r echo=FALSE, fig.cap="Rows with Residuals"}
resid <- augment(fitTest, h2_partition$test) %>% 
  mutate(obs_index = row_number()) %>%
  ggplot(aes(x=obs_index, y = .resid)) +
  geom_point() +
  geom_hline(aes(yintercept = 0)) + # add line at zero
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) # rotates and aligns labels

resid
```

\newpage

##### Plot of the Rows and Their .hat Values \newline

This graph shows the row values with their .hat values. We use these graphs to investigate influence points.

```{r echo=FALSE, fig.cap="Rows with .hat"}
hat <- augment(fitTest, h2_partition$test) %>% 
  mutate(obs_index = row_number()) %>%
  ggplot(aes(x=obs_index, y = .hat)) +
  geom_point() +
  geom_hline(aes(yintercept = 0)) + # add line at zero
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) # rotates and aligns labels

hat
```
Next, we wanted to pull out the highest values for each .cooksd, .resid, and .hat, as we can potentially use these to indicate influential points.

```{r, echo = F}
augment(fitTest, h2_partition$test) %>% 
  pivot_longer(cols = c(.resid, .hat, .cooksd)) %>%
  group_by(name) %>%
  slice_max(order_by = abs(value), n = 1) %>% 
  select(name, value)
```

```{r, echo = F, fig.width = 8, include=FALSE}
augment(fitTest) #row 6 .hat, row 51 resid
```
After investigation, we found that the data with the highest .hat value is located in row 6 of our test data, and the data with the highest .resid is located in the 51st row of our test data.

#### Comparing Models With and Without the Unusal Observations 

```{r echo=FALSE}
h2_testDF <- as.data.frame(h2_partition$test)
h2_testDF_removed <- h2_testDF[-c(6,51), ]
fitTest_removed <- lm(log(SalePrice) ~ ExterQual + LotArea + GarageCars + BsmtFinSF1 + HalfBath + FullBath + KitchenQual + OpenPorchSF + GarageFinish, data = h2_testDF_removed)
summary(fitTest)
summary(fitTest_removed)
```

After removing the high leverage point at row 6 and the outlier point at row 51, we fit the selected model with 9 variables again. By comparing the summary of the two models, we found that the adjusted $R^2$ and the F statistics increased slightly, which makes sense because the model has a better fit without the usual observations. However, the estimated coefficients of the variables and their significance did not change a lot, which means that removing unusual observations does not drastically change the model. 

#### Confidence Interval for Mean Response 

Next, we computed a confidence interval for the mean response. 

```{r echo=FALSE}
h2_testDF <- as.data.frame(h2_partition$test)
x_bar <- h2_testDF %>% select(c(LotArea, GarageCars, BsmtFinSF1, HalfBath, FullBath, OpenPorchSF)) %>% summarise(across(everything(), mean))
# finding most frequent values for the categorical variables 
#names(which.max(table(h2_testDF$ExterQual)))
#names(which.max(table(h2_testDF$KitchenQual)))
#names(which.max(table(h2_testDF$GarageFinish)))
# use the most frequent values in the mean prediction 
x_bar$ExterQual <- "TA"
x_bar$KitchenQual <- "TA"
x_bar$GarageFinish <- "Unf"
predict(fitTest, newdata = x_bar, interval = 'confidence', level = 0.95)
```

#### Prediction Interval At a Value 

Finally, we computed prediction intervals for a few points. The first is at a randomly chosen row, row 100. The next two are rows 6 and 51, the points of influence.

```{r echo=FALSE}
x_point <- h2_testDF[100, ]
predict(fitTest, newdata = x_point , interval = 'prediction', level = 0.95)
x_InfluencePt1 <- h2_testDF[6, ]
predict(fitTest, newdata = x_InfluencePt1 , interval = 'prediction', level = 0.95)
x_InfluencePt2 <- h2_testDF[51, ]
predict(fitTest, newdata = x_InfluencePt2 , interval = 'prediction', level = 0.95)
```
As you can see, the points of influence affect the prediction intervals slightly. 

#### Conclusion

For step 3 of our project, we first put aside test data and re-introduced the reader to our data source. 

Before we started our analysis of the data through models, we created pairs plots, and from these, we concluded that the data have a moderate correlation. 

For each of our models, we applied diagnostic techniques and transformed the response with the log function, in order to obtain a more constant variance. This allows us to view any correlation or relationships between our response and our explanatory variables better, with a more accurate model.

We chose two computational models, one including all of our numerical variables and one that we based off of correlation, including a categorical variable. 

We have then created an ANOVA table to cross-validate these models. 

We are not using interaction variables, as there is not sufficient evidence showing that variables interact. 

Next, we did statistical analysis to determine a good model. Keeping our response the log of SalePrice, we have performed forward and backward selection using AIC as our criteria. Both forward and backward selection returned us the same model that has the same 9 predictors, so we decided to use those to define our model. The variables chosen are ExterQual, LotArea, GarageCars, BsmtFinSF1, HalfBath, FullBath, KitchenQual, OpenPorchSF, and GarageFinish. 

After choosing our model, we have fit the model and looked at the corresponding R^2 value, which showed that 78% variation in the response was explained by the model we chose. 

We then fit our model on our test data and looked for influence points. To do so, we looked at the residual and hat matrix values. Those with high values in each respective category are influence points. 

Due to the potential influence of these points on our overall model, we have investigated the fit when removing unwanted data points and without. We concluded that these points do have some impact on the fit of our model, however, they do not drastically change our results. 

Finally, we computed a confidence interval for a mean predicted value and several prediction intervals for future predicted values at specific points. 

For our confidence interval, it is important to note that for the categorical variables, we found the most commonly occurring values of each.  We have used the variable ExterQual as "TA", the variable KitchenQual as "TA", and the variable GarageFinish as "Unf". 

In the prediction intervals, we tested each of our leverage points in addition to a general point that followed the trend, in order to see if there was much difference. We found that these points will change the interval, however there is not a huge discrepancy. 

The values we obtained from the confidence and prediction intervals were within our expectations in accordance with our model. 
Overall, we believe that the model we chose is a good fit for our data. We can conclude from our analysis that the most significant predictors are external quality(ExterQual), LotArea, GarageCars, finished basement square footage (BsmtFinSF1), HalfBath, FullBath, kitchen quality (KitchenQual), open porch square footage (OpenPorchSF), and GarageFinish.

