---
title: "Step-3"
author: "Liuqian Bao"
date: "2023-11-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("skimr")
library("dplyr")
options(max.print=1000000)
library("readxl")
library("ggplot2")
# load packages
library("tidyverse")
library("tidymodels")
library("modelr")
library("ggplot2")
library("GGally")
train <- read_excel("C:/Users/baoli/Desktop/PSTAT126/Project/house-prices-advanced-regression-techniques/train.xlsx") 
```

```{r include=FALSE}
# Variable Selection
h1 <- subset(train, select = -c(MSSubClass, OverallCond, BsmtFinSF2, LowQualFinSF, BsmtHalfBath, YrSold, MoSold, MiscVal, PoolArea, ScreenPorch, EnclosedPorch, KitchenAbvGr, BedroomAbvGr, Street, Alley, Utilities, LandSlope, RoofMatl, BsmtExposure, Heating, CentralAir, MiscFeature, Fence, PoolQC, LandContour,BldgType, Exterior2nd, GarageQual, GarageCond, PavedDrive, SaleType, SaleCondition, GrLivArea, OverallQual, TotalBsmtSF, WoodDeckSF, GarageArea, Fireplaces, TotRmsAbvGrd, MSZoning, Functional, MasVnrArea, GarageYrBlt, YearBuilt, YearRemodAdd))

h2 <- subset(train, select = c(SalePrice, LotArea, GarageCars, BsmtFinSF1, FullBath, ExterQual, HouseStyle, OpenPorchSF, HalfBath, KitchenQual, GarageFinish))
```

```{r echo=F}
# randomly choose 500 observations
set.seed(12345)

h2_500 <- h2[sample(1:1460, 500, replace = FALSE),]
h2_partition <- h2_500 %>% resample_partition(p = c(train = 0.7, test = 0.3)) ##fit model
h3_500 <- subset(h2_500, select = c(SalePrice, LotArea, GarageCars, BsmtFinSF1, FullBath, ExterQual)) ##pairplot
h4_500 <- subset(h2_500, select = c(HouseStyle, OpenPorchSF, HalfBath, KitchenQual, GarageFinish)) ##pairplot
```

```{r}
ggpairs(train3_500)
ggpairs(train4_500)

# plot(pairplot[2,1])
# plot(pairplot[2,2])
# plot(pairplot[2,3])
# plot(pairplot[2,4])
# plot(pairplot[2,5])
# plot(pairplot[2,6])
# plot(pairplot[2,7])
# plot(pairplot[2,8])
# plot(pairplot[2,9])
# plot(pairplot[2,10])
```
## Computational Models
First computational model: all of our quantitative variables.
```{r}
fitComp1 <- lm(log(SalePrice) ~ LotArea + GarageCars + BsmtFinSF1 + FullBath + OpenPorchSF + HalfBath, data = h2_partition$train)
summary(fitComp1)
```

```{r, echo=F, fig.width=8, fig.height=8, fig.cap="Residual vs. fitted and BsmtFinSF1"}
# panel of residual plots
augment(fitComp1, h2_partition$train) %>%
  pivot_longer(cols = c(.fitted, LotArea, GarageCars, BsmtFinSF1, FullBath, OpenPorchSF, HalfBath)) %>%
  ggplot(aes(y = .resid, x = value)) +
  facet_wrap(~ name, scales = 'free_x') +
  geom_point() +
  geom_hline(aes(yintercept = 0)) +
  geom_smooth(method = 'loess', formula = 'y ~ x', se = F, span = 1)
```

We tranformed SalePrice with a log function to make the variance more constant. 

Second computational model: we used GarageCars because it looked like it had high correlation, and KitchenQual becuase we wanted to test a highly correlated categorical variable. 

```{r}
fitComp2 <- lm(log(SalePrice) ~ GarageCars + KitchenQual, data = h2_partition$train)
summary(fitComp2)
```

We tranformed SalePrice with a log function to make the variance more constant.

## Anova to Cross Validate

```{r}
anova(fitComp1, fitComp2)
```

(we can also compare r^2 values)


#### Statistical model

```{r}
model_null = lm(SalePrice ~ 1, data = h2_partition$train)
model_full = lm(SalePrice ~ ., data = h2_partition$train)
# need to specify the scope of models to be examined
stats::step(model_full, direction="backward", test="F")
```

```{r}
stats::step(model_null, direction="forward", scope=list(upper=model_full, lower=model_null))
```
We performed both forward and backward selection, keeping the response as the log of SalePrice. We used the AIC as the criteria. The model returned will hace the predictors with the lowest AIC's. Each of these methods of selection returned the same model with 9 predictors, so we will use that as our statistical model.

(Using his method from lecture)
```{r, include=FALSE}
library(leaps)
out <- regsubsets(log(SalePrice) ~ ., data = h2_partition$train, # just like lm()
method = 'seqrep', # search strategy
nbest = 1, # how many models of each size?
nvmax = 10) # maximum number of predictors

summary(out)
```

### After Chose Model, fit it
```{r}
fitStat <- lm(log(SalePrice) ~ ExterQual + LotArea + GarageCars + BsmtFinSF1 + HalfBath + FullBath + KitchenQual + OpenPorchSF + GarageFinish, data = h2_partition$train)
summary(fitStat)
```

Looking at our R^2 value for this model shows that 78% of the variation in the response is explained by this model.

The following shows the residual plots we investigated to ensure that taking the log of the response helps with the model assumptions. 

```{r, echo=F, fig.width=8, fig.height=8, fig.cap="Residual vs. fitted and BsmtFinSF1"}
# panel of residual plots
augment(fitStat, h2_partition$train) %>%
  pivot_longer(cols = c(.fitted, LotArea, GarageCars, BsmtFinSF1, HalfBath, FullBath, OpenPorchSF)) %>%
  ggplot(aes(y = .resid, x = value)) +
  facet_wrap(~ name, scales = 'free_x') +
  geom_point() +
  geom_hline(aes(yintercept = 0)) +
  geom_smooth(method = 'loess', formula = 'y ~ x', se = F, span = 1)
```

### Reporting on Test Data

```{r}
fitTest <- lm(log(SalePrice) ~ ExterQual + LotArea + GarageCars + BsmtFinSF1 + HalfBath + FullBath + KitchenQual + OpenPorchSF + GarageFinish, data = h2_partition$test)
summary(fitTest)
```
### Looking for Influence points

```{r}
augment(fitTest, h2_partition$test) %>%
  mutate(obs_index = row_number()) %>%
  ggplot(aes(x=obs_index, y = .resid)) +
  geom_point() +
  geom_hline(aes(yintercept = 0)) + # add line at zero
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) # rotates and aligns labels
```
```{r, echo = T, fig.width = 10, fig.height = 8}
p_caseinf <- augment(fitTest, h2_partition$test) %>%
  pivot_longer(cols = c(.resid, .hat, .cooksd)) %>%
  mutate(obs_index = row_number()) %>%
  ggplot(aes(x=obs_index, y = value)) +
  facet_wrap(~ name, scales = 'free_y', nrow = 3) + # looks better with vertical faceting
  geom_point() +
  geom_hline(aes(yintercept = 0)) + # add line at zero
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) + # rotates and aligns labels
  labs(x = '', y = '') 

p_caseinf
```

```{r, echo = T}
augment(fitTest, h2_partition$test) %>% 
  pivot_longer(cols = c(.resid, .hat, .cooksd)) %>%
  group_by(name) %>%
  slice_max(order_by = abs(value), n = 1) %>% # play here
  select(name, value)
```

```{r, echo = T, fig.width = 8}
augment(fitTest) #row 6 .hat, row 51 resid
```

#### Confidence Interval for Mean
```{r}
fitTest %>% confint() # coefficients
```
```{r}
predict(fitTest, interval= 'confidence') # each value
```


For step 3 of our project, we first put aside test data and introduced the reader to our data source. 
Before we started our analysis of the data, we created the pairs plot, and from the pairs plot, we concluded that the data have a moderate correlation. 
For our model, we have used the log transformation in order to obtain better correlation and stronger, more reliable relationships with our explanatory variables by through the presence of a more constant variance. 
We have then created an ANOVA table to cross-validate our data, and we are not using interaction variables. 
Then we would choose two models that are of interest to us, which would be model_null and model_full. 
Keeping our response the log of SalePrice, we have performed forward and backward selection using AIC as our criteria. 
Both forward and backward selection returned us the same model that has the same 9 predictors, so we decided to use that as our model. 
After choosing our model, we have fit the model and looked at the corresponding R^2 value, which showed that 78% variation in the response was explained by the model we chose. 
In order to get better interpretation of the model, we created residual plots for our predictors to make sure that taking the log of our response SalePrice helps with our model assumptions.   
We then fitted our model and then we looked for influence points. We used techniques including vertical faceting to make our models look better, and we concluded that from our plots, we can see an outlier point that is far apart from the rest of the data. 
Due to the potential negative influence of the outlier on our overall model, we have fitted a model by removing unwanted data points and concluded that the new fitted model worked much better for the training and the test datasets. 
Due to the diversity of our predictors, we do have some predictors that are categorial variables which contain various levels, and these categorial variables had distinctive influences on our model. 
They also had influence for the mean of the confidence intervals we tested. Finally, we gave confidence intervals for a mean predicted value and a prediction interval for a future predicted value. 
For both the confidence interval and the prediction interval, we used the predict function, and we have used the mean of our dataset through selecting some notable variables for our mean confidence interval. 
The values we obtained from the confidence and prediction intervals were within our expectations in accordance with our model. 
Overall, we believed that the model we chose we a good fit for our data in step 3 of our project.
