---
title: "Step4Shrinkage"
author: "Liuqian Bao, Stella Ramirez, Andrew Cheng"
date: "2023-10-27"
output:
  pdf_document: default
  html_document: default
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# load packages
library(tidyverse)
library(tidymodels)
library(modelr)
install.packages("glmnet")
library(glmnet)
```

```{r, include=FALSE}
library("skimr")
library("dplyr")
options(max.print=1000000)
library(readxl)
library(ggplot2)
train <- read_excel("C:/Users/baoli/Desktop/PSTAT126/Project/house-prices-advanced-regression-techniques/train.xlsx")
```

Introduction: 

In this step, we will use data we have used for our previous steps and incorporate the shrinkage methods. 

The citation for our original data source is: Anna Montoya, DataCanary.
(2016).House Prices - Advanced Regression Techniques.
Kaggle.Obtained from <https://kaggle.com/competitions/house-prices-advanced-regression-techniques> ([https://kaggle.com/competitions/house-pricesadvanced-regression-techniques](https://kaggle.com/competitions/house-pricesadvanced-regression-techniques){.uri})

The population that we are inferring our results on are all residential houses in Ames, Iowa.

For step 4 of our project, we will execute ridge regression and LASSO on our dataset in order to construct the graph with both our observed response variable and our predicted response variable. 


```{r include=FALSE}
# Variable Selection
h1 <- subset(train, select = -c(MSSubClass, OverallCond, BsmtFinSF2, LowQualFinSF, BsmtHalfBath, YrSold, MoSold, MiscVal, PoolArea, ScreenPorch, EnclosedPorch, KitchenAbvGr, BedroomAbvGr, Street, Alley, Utilities, LandSlope, RoofMatl, BsmtExposure, Heating, CentralAir, MiscFeature, Fence, PoolQC, LandContour,BldgType, Exterior2nd, GarageQual, GarageCond, PavedDrive, SaleType, SaleCondition, GrLivArea, OverallQual, TotalBsmtSF, WoodDeckSF, GarageArea, Fireplaces, TotRmsAbvGrd, MSZoning, Functional, MasVnrArea, GarageYrBlt, YearBuilt, YearRemodAdd))

h2 <- subset(train, select = c(SalePrice, LotArea, GarageCars, BsmtFinSF1, FullBath, ExterQual, HouseStyle, OpenPorchSF, HalfBath, KitchenQual, GarageFinish))
```

```{r} 
h2[,-c(1,6,7,10, 11)]
round(cor(h2[,-c(1,6,7,10, 11)]),2)
```

## Ridge Regression

In order to use the glmnet package, we need to ensure that our response is a vector of log(SalePrice), and our predictors are in the form of a data.matrix.

```{r}
y <- log(h2$SalePrice)
x <- data.matrix(h2[, c('LotArea','GarageCars','BsmtFinSF1','FullBath','ExterQual','OpenPorchSF','HalfBath', 'KitchenQual')])
```

### Fit Ridge Regression Model

Next, we use the glmnet package to fit a model for ridge regression. The glmnet function automatically standardizes our predictor variables. This standardization makes it so that the standard deviation of each variable is 1 and then mean is 0.

```{r}
library(glmnet)

#fit ridge regression model
model <- glmnet(x, y, alpha = 0)

#view summary of model
summary(model)
```
### Finding Optimal Lambda

Next, we use cross validation to find the best lambda value. 

```{r}
#perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(x, y, alpha = 0)

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_lambda
```


The following shows a plot testing the MSE with the lambda values.

```{r}
#produce plot of test MSE by lambda value
plot(cv_model) 
```


Next, we view the estimates of the coefficients for this model.

```{r}
#find coefficients of best model
best_model <- glmnet(x, y, alpha = 0, lambda = best_lambda)
coef(best_model)
```

So that we could explore ridge regression further, and view what happens as lambda increases, we created the following graph.

```{r}
#Ridge trace plot
plot(model, xvar = "lambda")
```
Finally, we wanted to see what the $r^2$ value is for our ridge regression model. 

```{r}
#use fitted best model to make predictions
y_predicted <- predict(model, s = best_lambda, newx = x)

#find SST and SSE
sst <- sum((y - mean(y))^2)
sse <- sum((y_predicted - y)^2)

#find R-Squared
rsq <- 1 - sse/sst
rsq
```

This value of $r^2$ means that the best model possible, found as a result of ridge regression, explains 71.32% of the variation in the response.





### Lasso Model

We executed Lasso regression on the complete data set with the final MLR model from the previous project task.

#### Finding Optimal Lambda

We used cross validation to find the optimal lambda value that minimizes the mean squared error.

```{r}
require(glmnet)
y2 <- log(h2$SalePrice)
x2 <- data.matrix(h2[,c("ExterQual", "LotArea", "GarageCars", "BsmtFinSF1", "HalfBath", "FullBath", "KitchenQual", "OpenPorchSF")])
cv_model_lasso <- cv.glmnet(x2, y2, alpha = 1)
#finding optimal lambda
best_lambda <- cv_model_lasso$lambda.min
best_lambda
```

The optimal $\lambda$ we found is 0.001232506, so we are going to tune our lasso model using this $\lambda$ value.

In the graph below we plotted $log(\lambda)$ agianst the mean squared error.

```{r}
plot(cv_model, cex=0.8)
```

#### Find Coefficients of Best Model

Next, we view the estimates of the coefficients for the lasso model.

```{r}
best_model_lasso <- glmnet(x2, y2, alpha = 1, lambda = best_lambda)
coef(best_model_lasso)
```

In our case, no coefficient is shrunk all the way to zero, but the coefficients of LotArea, BsmtFinSF1, and OpenPorchSF are very close to zero, which means that they have close to no influence in our lasso model. 

#### Plot Best Coefficients

In the graph below we plotted estimates of the coefficients of all the variables in the lasso model.

```{r}
plot(coef(best_model_lasso), type="h", xlab="index", ylab="Coefficient", cex=0.8)
```

We can see in the graph that the coefficients are very small for LotArea, BsmtFinSF1, and OpenPorchSF. 
