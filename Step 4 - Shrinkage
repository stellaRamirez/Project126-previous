---
title: "Step4Shrinkage"
author: "Liuqian Bao, Stella Ramirez, Andrew Cheng"
date: "2023-10-27"
output:
  pdf_document: default
  html_document: default
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# load packages
library(tidyverse)
library(tidymodels)
library(modelr)
install.packages("glmnet")
library(glmnet)
```

```{r, include=FALSE}
library("skimr")
library("dplyr")
options(max.print=1000000)
library(readxl)
library(ggplot2)
train <- read_excel("C:/Users/baoli/Desktop/PSTAT126/Project/house-prices-advanced-regression-techniques/train.xlsx")
```

Introduction: 

In this step, we will use data we have used for our previous steps and incorporate the shrinkage methods. 

The citation for our original data source is: Anna Montoya, DataCanary.
(2016).House Prices - Advanced Regression Techniques.
Kaggle.Obtained from <https://kaggle.com/competitions/house-prices-advanced-regression-techniques> ([https://kaggle.com/competitions/house-pricesadvanced-regression-techniques](https://kaggle.com/competitions/house-pricesadvanced-regression-techniques){.uri})

The population that we are inferring our results on are all residential houses in Ames, Iowa.

For step 4 of our project, we will execute ridge regression and LASSO on our dataset in order to construct the graph with both our observed response variable and our predicted response variable. 


```{r include=FALSE}
# Variable Selection
h1 <- subset(train, select = -c(MSSubClass, OverallCond, BsmtFinSF2, LowQualFinSF, BsmtHalfBath, YrSold, MoSold, MiscVal, PoolArea, ScreenPorch, EnclosedPorch, KitchenAbvGr, BedroomAbvGr, Street, Alley, Utilities, LandSlope, RoofMatl, BsmtExposure, Heating, CentralAir, MiscFeature, Fence, PoolQC, LandContour,BldgType, Exterior2nd, GarageQual, GarageCond, PavedDrive, SaleType, SaleCondition, GrLivArea, OverallQual, TotalBsmtSF, WoodDeckSF, GarageArea, Fireplaces, TotRmsAbvGrd, MSZoning, Functional, MasVnrArea, GarageYrBlt, YearBuilt, YearRemodAdd))

h2 <- subset(train, select = c(SalePrice, LotArea, GarageCars, BsmtFinSF1, FullBath, ExterQual, HouseStyle, OpenPorchSF, HalfBath, KitchenQual, GarageFinish))
```

```{r} 
h2[,-c(1,6,7,10, 11)]
round(cor(h2[,-c(1,6,7,10, 11)]),2)
```

## Ridge Regression

In order to use the glmnet package, we need to ensure that our response is a vector of log(SalePrice), and our predictors are in the form of a data.matrix.

```{r}
y <- log(h2$SalePrice)
x <- data.matrix(h2[, c('LotArea','GarageCars','BsmtFinSF1','FullBath','ExterQual','OpenPorchSF','HalfBath', 'KitchenQual')])
```

### Fit Ridge Regression Model

Next, we use the glmnet package to fit a model for ridge regression. The glmnet function automatically standardizes our predictor variables. This standardization makes it so that the standard deviation of each variable is 1 and then mean is 0.

```{r}
library(glmnet)

#fit ridge regression model
model <- glmnet(x, y, alpha = 0)

#view summary of model
summary(model)
```
### Finding Optimal Lambda

Next, we use cross validation to find the best lambda value. 

```{r}
#perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(x, y, alpha = 0)

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_lambda
```


The following shows a plot testing the MSE with the lambda values.

```{r}
#produce plot of test MSE by lambda value
plot(cv_model) 
```


Next, we view the estimates of the coefficients for this model.

```{r}
#find coefficients of best model
best_model <- glmnet(x, y, alpha = 0, lambda = best_lambda)
coef(best_model)
```

So that we could explore ridge regression further, and view what happens as lambda increases, we created the following graph.

```{r}
#Ridge trace plot
plot(model, xvar = "lambda")
```
Finally, we wanted to see what the $r^2$ value is for our ridge regression model. 

```{r}
#use fitted best model to make predictions
y_predicted <- predict(model, s = best_lambda, newx = x)

#find SST and SSE
sst <- sum((y - mean(y))^2)
sse <- sum((y_predicted - y)^2)

#find R-Squared
rsq <- 1 - sse/sst
rsq
```

This value of $r^2$ means that the best model possible, found as a result of ridge regression, explains 71.32% of the variation in the response.
