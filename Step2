---
title: "PSTAT126 Project Step-2"
author: "Liuqian Bao"
date: "2023-10-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# load packages
library(tidyverse)
library(tidymodels)
library(modelr)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("skimr")
library("dplyr")
options(max.print=1000000)
library(readxl)
library(ggplot2)
train <- read_excel("C:/Users/baoli/Desktop/PSTAT126/house-prices-advanced-regression-techniques/train.xlsx") 
```

Introduction: We are using the variables BsmtFinSF1 and SalePrice as our variables of interest. 
The BsmtFinSF1 variable is our predictor variable that we will use for hypothesis testing and plotting.
The BsmtFinSF1 variable refers to the basement finished area square feet in the overall housing data.
The SalePrice variable refers to the property's sale price in dollars, and it is our response, or dependent, variable that is affected by BsmtFinSF1.
The original data source is from House Prices - Advanced Regression Techniques from the year 2016 found on Kaggle.
The population that we are inferring our results on are from every property sampled in our data set that is from all residential houses in Ames, Iowa.
We have done the log-transformation in order to better fit our model.


Hypothesis: Our hypothesis will be based on our predictor variable, BsmtFinSF1, and the response variable, SalePrice. Our null hypothesis is that BsmtFinSF1 
and SalePrice are not linearly correlated, beta_1 = 0. Our alternative hypothesis is that BsmtFinSF1 and SalePrice are positively correlated, beta_1 > 0.

```{r include=FALSE}
# Variable Selection
train1 <- subset(train, select = -c(MSSubClass, OverallCond, BsmtFinSF2, LowQualFinSF, BsmtHalfBath, YrSold, MoSold, MiscVal, PoolArea, ScreenPorch, EnclosedPorch, KitchenAbvGr, BedroomAbvGr, Street, Alley, Utilities, LandSlope, RoofMatl, BsmtExposure, Heating, CentralAir, MiscFeature, Fence, PoolQC, LandContour,BldgType, Exterior2nd, GarageQual, GarageCond, PavedDrive, SaleType, SaleCondition, GrLivArea, OverallQual, TotalBsmtSF, WoodDeckSF, GarageArea, Fireplaces, TotRmsAbvGrd, MSZoning, Functional, MasVnrArea, GarageYrBlt, YearBuilt, YearRemodAdd))

drop <- c("Condition1","Condition2", "3SsnPorch", "LotFrontage", "1stFlrSF", "2ndFlrSF")
train2 <- train1[,!(names(train1) %in% drop)]
train2
```

```{r}
set.seed(12345)
train2_500 <- train2[sample(1:1460, 500, replace = FALSE),]
```


```{r}
ggplot(data = train2_500, mapping = aes(x = BsmtFinSF1, y = SalePrice)) + 
  geom_point(alpha = 0.1) +
  labs(title="SalePrice vs. GarageCars") +
  xlab("GarageCars") + ylab("SalePrice")
```

#### Linear Model
```{r}
fit <- lm(SalePrice ~ ., train2_500)
fit_BsmtFinSF1 <- lm(SalePrice ~ BsmtFinSF1, train2_500)
summary(fit_BsmtFinSF1)
augment(fit_BsmtFinSF1, train2_500) %>% head(4)
```

```{r, echo = T, fig.width = 8, fig.height = 3}
# panel of residual plots
augment(fit_BsmtFinSF1, train2_500) %>%
  pivot_longer(cols = c(.fitted, BsmtFinSF1)) %>%
  ggplot(aes(y = .resid, x = value)) +
  facet_wrap(~ name, scales = 'free_x') +
  geom_point() +
  geom_hline(aes(yintercept = 0)) +
  geom_smooth(method = 'loess', formula = 'y ~ x', se = F, span = 1)
```

```{r}
# normality check (WITHOUT QUADRATIC)
augment(fit_BsmtFinSF1, train2_500) %>%
  ggplot(aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line()
```

#### t-test

```{r}
#finding p value
summary(fit_BsmtFinSF1)$coef
```
We are doing a one sided test, so will divide this by 2.

####confidence interval for beta1(BsmtFinSF1)
```{r}
confint(fit_BsmtFinSF1, 'BsmtFinSF1', level = 0.95)
```

With 95% confidence, a 1 square foot increase in basement square feet is associated with an increase in average sales prices between an estimated 58.24 and 89.14.


####r squared
```{r}
summary(fit_BsmtFinSF1)$adj.r.squared
```

Our model has an Adjusted R-squared value of 0.1482. This tells us that 14.82% of the variance in SalePrice is explained by our simple linear model with the explanatory 
variable basement finished area(BsmtFinSF1). This means our model explains a relatively  small amount of the variance, but it is reasonable considering we are only 
using one variable. 



In the following section is our original attempt to use the poly function to adjust linearity. However, we found this is unneeded in this case as the ggplots are very similar.

```{r, echo = T}
# add quadratic term in expenditure
fit_BsmtFinSF1_q <- lm(SalePrice ~ poly(BsmtFinSF1, 2, raw = T), data = train2_500)
```


```{r}
# normality check
augment(fit_BsmtFinSF1_q, train2_500) %>%
  ggplot(aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line()
```


#### Plot transformed
```{r}
# ggplot(data = train2_500, mapping = aes(x = poly(BsmtFinSF1, 2, raw = T), y = SalePrice))+
#   geom_point(alpha = 0.1) +
#   labs(title="SalePrice vs. GarageCars") +
#   xlab("GarageCars") + ylab("SalePrice")
```

```{r}
n <- dim(train2_500)[1] # number of observations, or equivalently use nrow(statedata)
p <- 1 # number of predictors
round(coefficients(summary(fit_BsmtFinSF1_q)), 5)
```



Summary: We have obtained the results of our hypothesis about whether we reject or fail to reject the hypothesis that B0=0. 
We then checked the assumptions for linear regression, with the results being  
Next, we computed the test and found the CI for B1, with the results being a 1 square foot increase in basement square feet being associated with an increase in average sales prices from 58.24 to 89.14 with 95 percent confidence. 
We then plotted our predictor variable with our response variable, with the CI for the mean and individual response being . 
We have done the log transformation to our model in order to better compare and fit our model, and to make our data as well as hypothesis testing more meaningful. 
We also have the fit of our model and the R^2 value for the residuals of our plot being an Adjusted R-squared value of 0.1482. 
The test of CI for B1 is of interest to us since an increase in average sales prices from 58.24 to 89.14 with 95 percent confidence is certainly notable given our data and residuals plot.  
Another interesting aspect was our Adjusted R-squared value of 0.1482 showing that 14.82% of the variance in SalePrice is explained by the simple linear model, meaning our model explained a relatively small amount of the variance. 
This is interesting information to us given the nature of our regression model as a single variable model, giving us validation that we were in the right direction.   
The data we obtained were mostly what we expected to get since the plot turned out to be along our expectations. 
Other questions that we would like to ask about the data is whether the data can be influenced by other variables. 
Also, another question of interest may be whether a transformation of the residuals plot by a function would impact the linearity and residuals of the plot. 
